\section{November 14, 2022}

\subsection{Classifying symmetric/Hermetian forms}

Our goal today is to get a better handle on classifying symmetric / Hermetian forms. Let $V$ be a finite-dimensional vector space over $\RR$ or $\CC$. Let $\gen{\cdot, \cdot}$ be a symmetric (if $V$ over $\RR$) or Hermetian (if $V$ over $\CC$) form. 

\begin{theorem}
\thmlabel

\begin{enumerate}
    \item [(1)] Let $W$ be a subspace of $V$. Then 
\[V = W\oplus W^{\perp}\iff \gen{\cdot, \cdot}\text{ non-deg on }W.\]
    \item [(2)] Let $W$ be a subspace of $V$. Then if $\gen{\cdot, \cdot}$ is non-deg on $V$ and $W$, it is not deg on $W^{\perp}$. 
\end{enumerate}
\end{theorem}

We proved (1) last lecture. Here, we prove (2). 

\begin{proof}
By (1), $V = W\oplus W^{\perp}$, so we can express the matrix for $\gen{\cdot, \cdot}$ as a block matrix
\[M = \left(
    \begin{array}{cc}
      \multicolumn{1}{c|}{A} & 0 \\ \cline{1-2}
      \multicolumn{1}{c|}{0} & B \\
    \end{array}
    \right)\]
where $A$ is the matrix for $\gen{\cdot, \cdot}$ on $W$, and $B$ is the matrix for $\gen{\cdot, \cdot}$ on $W^{\perp}$. Because our operator is non-deg on both $V$ and $W$, $M$ and $A$ both have non-zero determinant. On the other hand, $\det M = \det A\cdot \det B$, so the determinant of $B$ is also non-zero; therefore, our form is non-deg on $W^{\perp}$.
\end{proof}

\begin{theorem}
\lemlabel

If $\gen{\cdot, \cdot}$ is not identically zero, then $\gen{v,v}\neq 0$ for some $v\in V$. 
\end{theorem}

\begin{proof}

Suppose $\gen{x,y}\neq 0$. Note that replacing $y$ with $cy$ for any $c\in \CC$ has the effect of rotating $\gen{x,y}$ in the complex plane by the argument of $c$. Therefore, we may assume $\gen{x,y}\in \RR$, since we can rotate it to be real if necessary. So, we may assume $\gen{x,y} = \gen{y,x}$. Then, 
\begin{align*}
    \gen{x+y, x+y} &= \gen{x,x} + \gen{y,y} + \gen{x,y} + \gen{y,x} \\
    &= \gen{x,x} + \gen{y,y} + \underbrace{2\gen{x,y}}_{\neq 0},
\end{align*}
which implies at least one of $\gen{x+y, x+y}$, $\gen{x,x}$, or $\gen{y,y}$ is not zero. 
\end{proof}

Note that this proof implicitly uses the polarization identity that we used in Lectures 12 and 13, i.e., $\gen{x,y} = (\gen{x+y, x+y} - \gen{x,x} - \gen{y,y})/2$. Here, we framed it differently because of the need to deal with Hermitian forms acting weird. 

\begin{theorem}
\thmlabel

$V$ has an orthogonal basis $b_1, \hdots, b_n$, i.e., $\gen{b_i, b_j} = 0$ for all $i\neq j$. 
\end{theorem}

\begin{proof}
We proceed with induction on $\dim V$. 

If $\gen{\cdot, \cdot}$ is identically zero, we're done, since any basis is automatically orthogonal. Otherwise, there exists some $v\in V$ with $\gen{v,v}\neq 0$. Let $W = \Span v$.

Then $\gen{\cdot, \cdot}$ is nondeg on the one-dimensional subspace $W$, which implies $V = W\oplus W^{\perp}$. By induction, $W^{\perp}$ has an orthogonal basis. Adding $v$ into our basis, we're finished. 
\end{proof}

\begin{theorem}
\corlabel

There exists an orthogonal basis $b_1, \hdots, b_n$ with $\vert b_i\vert \in \{0,\pm 1\}$ for all $i$.
\end{theorem}

\begin{proof}
Take any orthogonal basis. For all $b_i$, if $\vert b_i\vert\neq 0$, we may replace it with $b_i/\sqrt{\vert \gen{b_i, b_i}\vert}$. 
\end{proof}

Note that this does not necessarily eliminate the case when $\gen{b_i, b_i} = -1$, since 
\[\left\vert b_i/\sqrt{\vert \gen{b_i, b_i}\vert} \right\vert = \gen{b_i, b_i}/\vert\gen{b_i,b_i}\vert\in \{\pm 1\}. \]

So, $\gen{\cdot, \cdot}$ has a basis with matrix 
\[
\begin{pmatrix}
I_{n_1} & 0 & 0 \\
0 & -I_{n_{-1}} & 0 \\
0 & 0 & 0
\end{pmatrix},
\]
where $n_t = \{\text{number of }b_i : \vert b_i\vert =t\}$, and $\dim V = n_0+n_1+n_{-1}$. In this basis,

\[\gen{x,y} = \sum_{i=1}^{n_1} \overline{x_i}y_i - \sum_{j=n_1+1}^{n_1 + n_{-1}} \overline{x_j}y_j.\] 

\subsection{Sylvester's Law of Inertia}

\begin{definition}
\deflabel

The \ac{signature} of the form $\gen{\cdot, \cdot}$ is the triple $(n_1, n_0, n_{-1})$.
\end{definition}

It turns out that this definition is well-defined by the following theorem. 

\begin{theorem}
\thmlabelname{Sylvester's Law of Inertia}

Each form $\gen{\cdot, \cdot}$ gives a unique signature which is independent of $b_1, \hdots, b_n$.
\end{theorem}

Prof. Cohn notes that Sylvester is responsible for lots of naming schemes in Algebra, and if we ever come across something that sounds strange, it was probably named by Sylvester. For example, he came up with ``determinant" in linear algebra, ``syzygy" in abstract algebra, and ``totient" in number theory, the latter of which is definitely just a made-up word that he used to sound fancy. The name of this particular theorem sounds crazy but he's using ``inertia" to mean unchanging, i.e., being inert, so it's not so bad.

Before proving the theorem, here is an interesting connection between Sylvester's Law of Inertia and a pretty fundamental result from multivariable calculus.

\begin{example}
\exlabel

Sylvester's Law of Inertia implies the second derivative test. Let $f: \RR^n\rightarrow \RR$ be twice continuously differentiable. 
\end{example}

Let $x_0$ be a critical point for $f$, i.e., $\nabla f(x_0) = 0$. From our previous discussions on the bilinear form representation of a second degree taylor polynomial, we can write
\[T_2(x) = f(x_0) + (x-x_0)^t H (x-x_0),\]
where $H$ is the \ac{Hessian matrix} given by
\[H = \left(\frac{\partial^2 f}{\partial x_i\partial x_j}(x_0)\right)_{1\leq i,j\leq n}.\]
Since partial derivatives commute, $H$ is symmetric. Therefore, Sylvester's Law of Inertia implies that we can choose coordinates so that $H$ is diagonal (with $n_1$ $1$s, $n_{-1}$ $-1$s, and $n_0$ $0$s on the diagonal), in which the second degree taylor polynomial becomes
\[f(x_0) + \sum_{i=1}^{n_1}x_i^2 - \sum_{j=n_1+1}^{n_1+n_{-1}}x_j^2.\]

When $n_0 = 0$, we say that $x_0$ is a nondegenerate critical point of $f$, because our matrix in this case is nondegenerate, since it is invertible. Our form is positive semidefinite if and only if $n_{-1} = 0$, and positive definite if and only if $n_0 = n_{-1} = 0$. 

Let's consider all nondegenerate critical points. When $n_{-1} = 0$, we have a local minimum, since shifting our coordinates can only increase the value of our taylor polynomial (positive definite). $n_{-1}=1$ gives us a saddle point with one direction of decrease, i.e., the one coordinate corresponding to the negative eigenvalue. It can also be thought of as a mountain pass between two minima, or a potential barrier between stable states given a reaction pathway from one state to another. $n_{-1}=2$ defines a saddle point with two directions of decrease, or a barrier to transform one reaction pathway to another. $n_{-1}=3$ defines a barrier to transform one transformation of a reaction pathway to another, to another transformation of a reaction pathway to another. And so on.

Just like the normal second derivative test, we can't say anything about degenerate critical points, because we do not have information about how $f$ behaves when we adjust coordinates corresponding to null eigenvalues. \comment{Add image}

\subsection{Proving Sylvester's Law of Inertia}

Now we prove Sylvester's Law of Inertia.

\begin{proof}
Let $V = V_1\oplus V_0\oplus V_{-1}$, each corresponding to subspace spanned by the three types of basis vectors. 

We first claim that $V_1$ is the max dimensional subspace on which our form is positive definite. $V_1$ is positive definite, because its matrix is just the identity. Any higher dimensional subspace has to intersect $V_0\oplus V_{-1}$, but every vector in $V_0\oplus V_{-1}$ has non-positive norm (i.e., negative semi-definite). Therefore, every subspace $W\subseteq V$ on which $\gen{\cdot, \cdot}$ is positive definite has $\dim W\leq n_1$. 

An analogous argument yields that $n_{-1}$ is the dimension of the largest subspace on which our form is negative definite, so every subspace $W\subseteq V$ on which $\gen{\cdot, \cdot}$ is negative definite has $\dim W \leq n_{-1}$.

So, suppose $V$ has two signatures $(n_1, n_0, n_{-1})$ and $(n_1', n_0', n_{-1}')$. Applying our inequalities from the first signature to the second gives $n_1\leq n_1'$ and $n_{-1}\leq n_{-1}'$. On the other hand, we can also apply them the other way, giving $n_1'\leq n_1$ and $n_{-1}'\leq n_{-1}$, so $n_1=n_1'$ and $n_{-1} = n_{-1}'$. Finally, $n_0=n_0'$ from $\dim V = n_0 + n_1 + n_{-1} = n_0' + n_1' + n_{-1}'$, so we're done. 
\end{proof}