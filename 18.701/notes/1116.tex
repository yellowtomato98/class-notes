\section{November 16, 2022}

\subsection{Euclidean/Hermetian spaces}

Last lecture, we classified all symmetric ($\RR$) / Hermetian ($\CC$) forms. In particular, we showed that there exists a basis such that the matrix for $\gen{\cdot, \cdot}$ is diagonal with entries $\pm 1, 0$. Moreover, Sylvester's Law of Inertia showed that each form gives a unique signature. 

\begin{definition}
\deflabel

A \ac{Euclidean space} is a finite dimensional real vector space with an inner product. A \ac{Hermetian space} is a finite dimensional complex vector space with a Hermetian inner product. 
\end{definition}

By our classification, there always exists an orthonormal basis for these spaces for which the matrix corresponding to the inner product is the identity. If $n_{-1} > 0$, then the form can't be positive semidefinite, and if $n_0 > 0$, then the form can't be positive definite. 

Therefore, inner products on these spaces are all the same as the usual dot product on $\RR^n$ or Hermetian inner product on $\CC^n$, up to a change of basis. In other words, $A\in \CC^{n\times n}$ gives a Hermetian inner product $\iff A = B^* I_n B = B^*B$ for some $B\in GL_n(\CC)$. ($B$ must be invertible, since changing the basis of a matrix is an isomorphism from our original vector space to itself.) The real case is analogous; we must have $A = B^tB$ for some $B\in GL_n(\RR)$. 

Prof. Cohn addresses the fact that it's probably confusing why we continue to separate real and complex cases when, for everything that we have been doing so far, the exact same results hold with the exact same proofs. This will no longer be the case once we get to spectral theory. 

\begin{theorem}
\thmlabelname{Sylvester's criterion}

Suppose $A\in \CC^{n\times n}$ and $A^*=A$. Then $A$ is positive definite if and only if for all $k=1, \hdots, n$,
\[\det (A_{ij})_{1\leq i,j\leq k} > 0.\]

The real case holds analogously. 
\end{theorem}

\begin{proof}
First, suppose that $A$ is positive definite, so it is equal to $B^*B$ for some $B\in GL_n(\CC)$ by our classification. Then,
\begin{align*}
    \det A &= \det (B^*) \det (B) \\
    &= \overline{\det (B)}\det (B) \\
    &= \vert \det (B)\vert ^2 > 0.
\end{align*}

Now consider what happens when we restrict our basis to the first $k$ vectors. Since $A$ was positive definite, the new $k\times k$ matrix formed by restricting our basis (in the upper left corner of $A$) must also be positive definite. Therefore, the same argument holds to show that the determinant for all such matrices are strictly positive. 

Now, suppose $\det (A_{ij})_{1\leq i,j\leq k} >0$ for all $k$. By assumption, $A_{11} > 0$. We can use row operations to zero out the rest of the first column in $A$, and then use column operations to zero out the rest of the first row in $A$. 

This is equivalent to transforming $A$ to a different basis. If we let $v_1, \hdots, v_n$ be our original basis, with $v_1$ corresponding to the row/column containing $A_{11}$, each row/column operation is equivalent to replacing the basis vector $v_i$ with $v_i+\alpha v_1$, with $\alpha$ chosen such that $\gen{v_i+\alpha v_1, v_1} = 0$. Therefore, our row/column operations has the effect of transforming $A\mapsto B^*AB$ for some $B\in GL_n(\CC)$. 

\[\overbrace{\left(
    \begin{array}{ccccc}
      A_{11} & \cdots &  \cdots &  \cdots & A_{1n} \\
      \vdots &&&&\\
      \vdots &&\ddots &&\\
      \vdots &&&&\\
      A_{n1} &&&&\\
    \end{array}
    \right)}^{\displaystyle v_1 \hspace{1.4cm} \hdots \hspace{1.4cm} v_n}
\mapsto
\left(
    \begin{array}{ccccc}
      A_{11} & 0 & \cdots & \multicolumn{1}{c|}{0} &\cdots \\
      \cline{2-5}
      \multicolumn{1}{c|}{0} &&&\multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} \\
      \multicolumn{1}{c|}{\vdots} && M_k &\multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} \\
      \multicolumn{1}{c|}{0} &&&\multicolumn{1}{c|}{} &\multicolumn{1}{c|}{}\\ \cline{1-4}
      \multicolumn{1}{c|}{\vdots} &&&& \multicolumn{1}{c|}{}\\
      \cline{2-5}
    \end{array}
    \right)\]

Our transformed matrix still satisfies the determinant property, since determinants are invariant to row and column operations. Let $M$ be the matrix in the lower right corner of $A$ after this transformation. Now, for any $M_k = (M_{ij})_{1\leq i,j\leq k}$, 
\[\det \left(\twotwo{A_{11}}{0}{0}{M_k}\right) > 0 \implies \det M_k > 0,\]
where the first inequality holds because our transformed $A$ still satisfies the determinant property. Therefore, $M$ also satisfies the determinant property, so we can induct downward to show that there exists a basis for $A$ under which it is diagonal with only positive elements along the diagonal. Since this is true if and only if $A$ is positive definite, we are done. 
\end{proof}

\begin{misconception}
\mislabel 

Suppose $A\in \CC^{n\times n}$ and $A^* = A$. Then $A$ is positive semidefinite if and only if for all $k = 1,\hdots, n,$
\[\det (A_{ij})_{1\leq i,j\leq k} \geq 0.\]
The real case holds analogously.
\end{misconception}

This seems like it really should be true, but unfortunately, it's not. Here is a concrete counterexample: 
\[A = \twotwo{0}{0}{0}{-1}.\]

$A$ is not positive semidefinite, since $n_{-1} > 0$. On the other hand, both determinants are non-negative, so the claim above must be false. The problem stems from the fact that we are no longer able to induct downwards in the same way that we did before;  $A_{11}\cdot \det M_k\geq 0$ tells us nothing about the sign of $\det M_k$ when $A_{11} = 0$. 

% Prof. Cohn is saying something about Paul Halmos, but I missed it. 

\subsection{Gram-Schmidt orthogonalization}

Suppose $V$ is Euclidean ($\RR$) / Hermetian ($\CC$). All subspaces are automatically non-degenerate; for any $W\subseteq V$, $w\in W$, and $w\neq 0$, then $\gen{w,w} > 0$. 

Tangent: special relativity operates on four dimensional vectors with signature $(3,0,1)$ so you have to worry about degeneracy, something about light cones. 

For any subspace $W\subseteq V$, let $W$ have orthogonal basis $w_1, \hdots, w_k$. We can orthogonally project $V$ to $W$ as follows. Since our form is non degenerate, $V = W\oplus W^{\perp}$, so we can write
\[v = \underbrace{c_1w_1+\hdots + c_kw_k}_{\in W} + \underbrace{u}_{\in W^{\perp}} \quad \forall v\in V.\]
Then, we can project $v$ to $v-u$.

How do we get an orthonormal basis for $W$ in the first place? We can use a process known as \ac{Gram-Schmidt orthogonalization}. Start with any basis $w_1, \hdots, w_k$. For each $i$ from $1$ to $k$, replace 
\[w_i \mapsto w_i - \sum_{j=1}^{i-1}\gen{w_j, w_i}w_j,\]
and then normalize $w_i$. After $i$ replacements, $\{w_1, \hdots, w_i\}$ are orthonormal. 

Here is a brief sketch for why this works. For each new basis vector $w_i$ that we add, we can express as some linear combination of the other (transformed) basis vectors, $\alpha_1w_1 + \hdots + \alpha_{i-1}w_{i-1}$. Then, 
\begin{align*}
    \gen{w_i - \sum_{k=1}^{i-1}\alpha_k w_k, w_j} = 0 &\iff \gen{w_i, w_j} - \alpha_j\gen{w_j, w_j} = 0 \\
    &\iff \alpha_j = \frac{\gen{w_i, w_j}}{\gen{w_j, w_j}} = \gen{w_i, w_j}
\end{align*}
must hold for all $1\leq j\leq i-1$. The last equality holds since we are normalizing our vectors after each iteration of the algorithm.

In matrix form, Gram-Schmidt orthogonalization takes a basis $v_1, \hdots, v_n$ of $\RR^n$ and transforms it into an orthonormal basis $u_1, \hdots, u_n$ such that $u_i\in \Span \{v_1, \hdots, v_i\}$ for all $i$. 

In other words, if we have 
\[V = \begin{pmatrix}
    \vert & & \vert \\
    v_1   & \hdots & v_n   \\
    \vert & & \vert
\end{pmatrix}\quad \text{and}\quad 
U = \begin{pmatrix}
    \vert & & \vert \\
    u_1   & \hdots & u_n   \\
    \vert & & \vert
\end{pmatrix},\]

then $U = VA$ for some upper triangular $A$ (by Gram-Schmidt). This means that we can factor any $V\in GL_n(\RR)$ as $UA^{-1}$ for some $U\in O_n(\RR)$ and $A^{-1}$ upper triangular. This is called \ac{QR factorization}. 